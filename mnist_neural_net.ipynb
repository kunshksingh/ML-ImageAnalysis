{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunshksingh/ML-ImageAnalysis/blob/main/mnist_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmKvZ4Jy_Ozq"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from sklearn.model_selection import ParameterGrid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
        "    the validation data, and the test data.\n",
        "    The ``training_data`` is returned as a tuple with two entries.\n",
        "    The first entry contains the actual training images.  This is a\n",
        "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
        "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "    pixels in a single MNIST image.\n",
        "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "    containing 50,000 entries.  Those entries are just the digit\n",
        "    values (0...9) for the corresponding images contained in the first\n",
        "    entry of the tuple.\n",
        "    The ``validation_data`` and ``test_data`` are similar, except\n",
        "    each contains only 10,000 images.\n",
        "    This is a nice data format, but for use in neural networks it's\n",
        "    helpful to modify the format of the ``training_data`` a little.\n",
        "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
        "    below.\n",
        "    \"\"\"\n",
        "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "eLVcUvta_dtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wrapper():\n",
        "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
        "    test_data)``. Based on ``load_data``, but the format is more\n",
        "    convenient for use in our implementation of neural networks.\n",
        "    In particular, ``training_data`` is a list containing 50,000\n",
        "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
        "    containing the input image.  ``y`` is a 10-dimensional\n",
        "    numpy.ndarray representing the unit vector corresponding to the\n",
        "    correct digit for ``x``.\n",
        "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
        "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
        "    numpy.ndarry containing the input image, and ``y`` is the\n",
        "    corresponding classification, i.e., the digit values (integers)\n",
        "    corresponding to ``x``.\n",
        "    Obviously, this means we're using slightly different formats for\n",
        "    the training data and the validation / test data.  These formats\n",
        "    turn out to be the most convenient for use in our neural network\n",
        "    code.\"\"\"\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n"
      ],
      "metadata": {
        "id": "vquo65Ln_dmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    (0...9) into a corresponding desired output from the neural\n",
        "    network.\"\"\"\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ],
      "metadata": {
        "id": "_DKkgkZs_di-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, dev, test = load_data_wrapper()\n",
        "training_data = list(train)\n",
        "validation_data = list(dev)\n",
        "test_data = list(test)"
      ],
      "metadata": {
        "id": "zAgTQJLr_kCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkARb9wIGbv2",
        "outputId": "af4fde5f-34e0-4228-ea5b-13632a05d564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.01171875],\n",
            "       [0.0703125 ],\n",
            "       [0.0703125 ],\n",
            "       [0.0703125 ],\n",
            "       [0.4921875 ],\n",
            "       [0.53125   ],\n",
            "       [0.68359375],\n",
            "       [0.1015625 ],\n",
            "       [0.6484375 ],\n",
            "       [0.99609375],\n",
            "       [0.96484375],\n",
            "       [0.49609375],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.1171875 ],\n",
            "       [0.140625  ],\n",
            "       [0.3671875 ],\n",
            "       [0.6015625 ],\n",
            "       [0.6640625 ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.87890625],\n",
            "       [0.671875  ],\n",
            "       [0.98828125],\n",
            "       [0.9453125 ],\n",
            "       [0.76171875],\n",
            "       [0.25      ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.19140625],\n",
            "       [0.9296875 ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98046875],\n",
            "       [0.36328125],\n",
            "       [0.3203125 ],\n",
            "       [0.3203125 ],\n",
            "       [0.21875   ],\n",
            "       [0.15234375],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.0703125 ],\n",
            "       [0.85546875],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.7734375 ],\n",
            "       [0.7109375 ],\n",
            "       [0.96484375],\n",
            "       [0.94140625],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.3125    ],\n",
            "       [0.609375  ],\n",
            "       [0.41796875],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.80078125],\n",
            "       [0.04296875],\n",
            "       [0.        ],\n",
            "       [0.16796875],\n",
            "       [0.6015625 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.0546875 ],\n",
            "       [0.00390625],\n",
            "       [0.6015625 ],\n",
            "       [0.98828125],\n",
            "       [0.3515625 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.54296875],\n",
            "       [0.98828125],\n",
            "       [0.7421875 ],\n",
            "       [0.0078125 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.04296875],\n",
            "       [0.7421875 ],\n",
            "       [0.98828125],\n",
            "       [0.2734375 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.13671875],\n",
            "       [0.94140625],\n",
            "       [0.87890625],\n",
            "       [0.625     ],\n",
            "       [0.421875  ],\n",
            "       [0.00390625],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.31640625],\n",
            "       [0.9375    ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.46484375],\n",
            "       [0.09765625],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.17578125],\n",
            "       [0.7265625 ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.5859375 ],\n",
            "       [0.10546875],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.0625    ],\n",
            "       [0.36328125],\n",
            "       [0.984375  ],\n",
            "       [0.98828125],\n",
            "       [0.73046875],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.97265625],\n",
            "       [0.98828125],\n",
            "       [0.97265625],\n",
            "       [0.25      ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.1796875 ],\n",
            "       [0.5078125 ],\n",
            "       [0.71484375],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.80859375],\n",
            "       [0.0078125 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.15234375],\n",
            "       [0.578125  ],\n",
            "       [0.89453125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.9765625 ],\n",
            "       [0.7109375 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.09375   ],\n",
            "       [0.4453125 ],\n",
            "       [0.86328125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.78515625],\n",
            "       [0.3046875 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.08984375],\n",
            "       [0.2578125 ],\n",
            "       [0.83203125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.7734375 ],\n",
            "       [0.31640625],\n",
            "       [0.0078125 ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.0703125 ],\n",
            "       [0.66796875],\n",
            "       [0.85546875],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.76171875],\n",
            "       [0.3125    ],\n",
            "       [0.03515625],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.21484375],\n",
            "       [0.671875  ],\n",
            "       [0.8828125 ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.953125  ],\n",
            "       [0.51953125],\n",
            "       [0.04296875],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.53125   ],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.98828125],\n",
            "       [0.828125  ],\n",
            "       [0.52734375],\n",
            "       [0.515625  ],\n",
            "       [0.0625    ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ],\n",
            "       [0.        ]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkSoftmax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        x = F.softmax(x, dim=1)         # Softmax activation function\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kK2UpcGf_y9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)                 # The model works better without Softmax\n",
        "        return x"
      ],
      "metadata": {
        "id": "EHPtSr_ldyW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkMini(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkMini, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64, 32)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(32, 10)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Dw-Jlju6EMgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkMax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkMax, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(256, 128)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(128, 10)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5AD0qRAiYOM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkWeighted(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkWeighted, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)    # Output layer\n",
        "        init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='sigmoid')\n",
        "        init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='sigmoid')\n",
        "        init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3gutRcJCEz_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkBias(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkBias, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)    # Output layer\n",
        "        init.constant_(self.fc1.bias, 0.1)\n",
        "        init.constant_(self.fc2.bias, 0.1)\n",
        "        init.constant_(self.fc3.bias, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ifeY6YgAFT9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkBiasHalf(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkBiasHalf, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)    # Output layer\n",
        "        init.constant_(self.fc1.bias, 0.5)\n",
        "        init.constant_(self.fc2.bias, 0.5)\n",
        "        init.constant_(self.fc3.bias, 0.5)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ET0mKMMfx6UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkThree(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkThree, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(256, 128)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(128, 64)   # Third hidden layer\n",
        "        self.fc4 = nn.Linear(64, 10)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation function\n",
        "        x = torch.sigmoid(self.fc3(x))  # Sigmoid activation function\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JjyWTxsDU9id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(data, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i + batch_size]\n",
        "        images, labels = zip(*batch)\n",
        "        images = np.array(images)\n",
        "        labels = np.array([np.argmax(y) for y in labels])\n",
        "\n",
        "        # Convert numpy arrays to tensors\n",
        "        images_tensor = torch.tensor(images).view(-1, 784)\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        yield images_tensor, labels_tensor"
      ],
      "metadata": {
        "id": "Wq0a98MgDK2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_grid = {\n",
        "    'batch_size': [1], # 1, 2, 4, 8, 16, 32, 64\n",
        "    'epochs': [20], # 1, 2, 4, 5, 8, 10, 20\n",
        "    'learning_rate': [0.01], # 0.1, 0.02, 0.01, 0.005, 0.001\n",
        "}"
      ],
      "metadata": {
        "id": "fmeHYPvETsHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, type=\"v\"):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  if type == \"t\":\n",
        "      all_data = test_data\n",
        "  else:\n",
        "      all_data = validation_data\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in all_data:\n",
        "          data = torch.tensor(data).view(-1, 784)\n",
        "          target = torch.tensor(target, dtype=torch.long)\n",
        "          outputs = model(data)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += 1\n",
        "          correct += (predicted == target).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  # print(f\"Accuracy: {accuracy}%\")\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "FeTxK2-eAaNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import ParamSpecKwargs\n",
        "def train_and_evaluate_model(params, model):\n",
        "    # model = NeuralNetwork()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'])\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "        for data, target in create_batches(training_data, params['batch_size']):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_function(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pass\n",
        "\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        accuracy = get_accuracy(model)\n",
        "        print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Accuracy: {accuracy}\")\n",
        "        # with torch.no_grad():\n",
        "        #     # Validate after each epoch\n",
        "        #     validation_loss = 0\n",
        "        #     for data, target in create_batches(validation_data, params['batch_size']):\n",
        "        #         output = model(data)\n",
        "        #         validation_loss += loss_function(output, target).item()\n",
        "        #     validation_loss /= len(validation_data)\n",
        "        #     print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {validation_loss}\")\n",
        "        ParamSpecKwargs\n",
        "    # accuracy = get_accuracy(model)\n",
        "    return accuracy, model\n"
      ],
      "metadata": {
        "id": "TAceyn-0UGzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grid of parameters\n",
        "grid = ParameterGrid(parameter_grid)\n",
        "\n",
        "# Iterate over all combinations of parameters\n",
        "print(\"----------Default----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetwork()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-enLhdO4n0",
        "outputId": "dbf661f9-e016-4827-8908-6db3d9dbef33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Default----------\n",
            "Epoch 1, Training Loss: 0.2626171112060547, Validation Accuracy: 90.12\n",
            "Epoch 2, Training Loss: 0.05944383889436722, Validation Accuracy: 93.5\n",
            "Epoch 3, Training Loss: 0.01809985563158989, Validation Accuracy: 95.27\n",
            "Epoch 4, Training Loss: 0.007947016507387161, Validation Accuracy: 96.0\n",
            "Epoch 5, Training Loss: 0.004364726599305868, Validation Accuracy: 96.5\n",
            "Epoch 6, Training Loss: 0.002749355509877205, Validation Accuracy: 96.71\n",
            "Epoch 7, Training Loss: 0.0018368767341598868, Validation Accuracy: 96.88\n",
            "Epoch 8, Training Loss: 0.0013763965107500553, Validation Accuracy: 96.98\n",
            "Epoch 9, Training Loss: 0.0011747133685275912, Validation Accuracy: 97.15\n",
            "Epoch 10, Training Loss: 0.0010706413304433227, Validation Accuracy: 97.16\n",
            "Epoch 11, Training Loss: 0.0009589364635758102, Validation Accuracy: 97.24\n",
            "Epoch 12, Training Loss: 0.0008156548719853163, Validation Accuracy: 97.33\n",
            "Epoch 13, Training Loss: 0.0006692553870379925, Validation Accuracy: 97.45\n",
            "Epoch 14, Training Loss: 0.0005392765742726624, Validation Accuracy: 97.52\n",
            "Epoch 15, Training Loss: 0.00043275527423247695, Validation Accuracy: 97.6\n",
            "Epoch 16, Training Loss: 0.0003667397249955684, Validation Accuracy: 97.62\n",
            "Epoch 17, Training Loss: 0.0003270567976869643, Validation Accuracy: 97.7\n",
            "Epoch 18, Training Loss: 0.0002829628065228462, Validation Accuracy: 97.75\n",
            "Epoch 19, Training Loss: 0.00023850933939684182, Validation Accuracy: 97.79\n",
            "Epoch 20, Training Loss: 0.000198821333469823, Validation Accuracy: 97.85\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.85\n",
            "Epoch 1, Training Loss: 2.436232328414917, Validation Accuracy: 34.5\n",
            "Epoch 2, Training Loss: 1.8522331714630127, Validation Accuracy: 65.96\n",
            "Epoch 3, Training Loss: 1.157428503036499, Validation Accuracy: 79.97\n",
            "Epoch 4, Training Loss: 0.8293260931968689, Validation Accuracy: 85.74\n",
            "Epoch 5, Training Loss: 0.5352104306221008, Validation Accuracy: 88.3\n",
            "Epoch 6, Training Loss: 0.35146403312683105, Validation Accuracy: 89.46\n",
            "Epoch 7, Training Loss: 0.23958930373191833, Validation Accuracy: 90.32\n",
            "Epoch 8, Training Loss: 0.17048898339271545, Validation Accuracy: 90.85\n",
            "Epoch 9, Training Loss: 0.12704764306545258, Validation Accuracy: 91.22\n",
            "Epoch 10, Training Loss: 0.0985105037689209, Validation Accuracy: 91.57\n",
            "Epoch 11, Training Loss: 0.07868526130914688, Validation Accuracy: 91.9\n",
            "Epoch 12, Training Loss: 0.06416064500808716, Validation Accuracy: 92.33\n",
            "Epoch 13, Training Loss: 0.05306467041373253, Validation Accuracy: 92.61\n",
            "Epoch 14, Training Loss: 0.04434452950954437, Validation Accuracy: 92.84\n",
            "Epoch 15, Training Loss: 0.03738201782107353, Validation Accuracy: 93.12\n",
            "Epoch 16, Training Loss: 0.031780656427145004, Validation Accuracy: 93.35\n",
            "Epoch 17, Training Loss: 0.027258284389972687, Validation Accuracy: 93.7\n",
            "Epoch 18, Training Loss: 0.023591885343194008, Validation Accuracy: 93.97\n",
            "Epoch 19, Training Loss: 0.020598655566573143, Validation Accuracy: 94.25\n",
            "Epoch 20, Training Loss: 0.01812795363366604, Validation Accuracy: 94.47\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.001}. Score: 94.47\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.85. Final Test Accuracy 97.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With Softmax\n",
        "grid = ParameterGrid(parameter_grid)\n",
        "\n",
        "# Iterate over all combinations of parameters\n",
        "print(\"----------Default----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkSoftmax()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7fxA5ObcsA",
        "outputId": "2e3ac40d-8d43-4576-c53b-f6e0ce9e5615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Default----------\n",
            "Epoch 1, Training Loss: 2.383850336074829, Validation Accuracy: 35.51\n",
            "Epoch 2, Training Loss: 2.4277496337890625, Validation Accuracy: 63.75\n",
            "Epoch 3, Training Loss: 2.4139959812164307, Validation Accuracy: 66.57\n",
            "Epoch 4, Training Loss: 1.6468204259872437, Validation Accuracy: 74.91\n",
            "Epoch 5, Training Loss: 1.4945931434631348, Validation Accuracy: 75.91\n",
            "Epoch 6, Training Loss: 1.470283031463623, Validation Accuracy: 76.43\n",
            "Epoch 7, Training Loss: 1.4653940200805664, Validation Accuracy: 76.75\n",
            "Epoch 8, Training Loss: 1.46372652053833, Validation Accuracy: 76.97\n",
            "Epoch 9, Training Loss: 1.4628885984420776, Validation Accuracy: 77.18\n",
            "Epoch 10, Training Loss: 1.4624370336532593, Validation Accuracy: 77.38\n",
            "Epoch 11, Training Loss: 1.4621838331222534, Validation Accuracy: 77.54\n",
            "Epoch 12, Training Loss: 1.4620383977890015, Validation Accuracy: 77.59\n",
            "Epoch 13, Training Loss: 1.461952567100525, Validation Accuracy: 77.6\n",
            "Epoch 14, Training Loss: 1.4618773460388184, Validation Accuracy: 77.73\n",
            "Epoch 15, Training Loss: 1.4617919921875, Validation Accuracy: 77.73\n",
            "Epoch 16, Training Loss: 1.4617058038711548, Validation Accuracy: 77.81\n",
            "Epoch 17, Training Loss: 1.4616422653198242, Validation Accuracy: 77.93\n",
            "Epoch 18, Training Loss: 1.461582899093628, Validation Accuracy: 78.02\n",
            "Epoch 19, Training Loss: 1.4615360498428345, Validation Accuracy: 78.1\n",
            "Epoch 20, Training Loss: 1.4614968299865723, Validation Accuracy: 78.23\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 78.23\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 78.23. Final Test Accuracy 77.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter2 = {\n",
        "    'batch_size': [1], # 1, 2, 4, 8, 16, 32, 64\n",
        "    'epochs': [20], # 1, 2, 4, 5, 8, 10, 20\n",
        "    'learning_rate': [0.01], # 0.1, 0.02, 0.01, 0.005, 0.001\n",
        "}"
      ],
      "metadata": {
        "id": "LTJ7PrAWa9QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other Models\n",
        "grid = ParameterGrid(parameter2)\n",
        "\n",
        "print(\"----------Mini----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkMini()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")\n",
        "\n",
        "print(\"----------Max----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkMax()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aA6NqQ8Za5Vk",
        "outputId": "27dac3de-5f93-4f83-b4d1-38261f252cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Mini----------\n",
            "Epoch 1, Training Loss: 0.2504449784755707, Validation Accuracy: 90.54\n",
            "Epoch 2, Training Loss: 0.04383499175310135, Validation Accuracy: 93.73\n",
            "Epoch 3, Training Loss: 0.01685473322868347, Validation Accuracy: 95.36\n",
            "Epoch 4, Training Loss: 0.010993153788149357, Validation Accuracy: 96.08\n",
            "Epoch 5, Training Loss: 0.008199954405426979, Validation Accuracy: 96.51\n",
            "Epoch 6, Training Loss: 0.006256043910980225, Validation Accuracy: 96.71\n",
            "Epoch 7, Training Loss: 0.0047610728070139885, Validation Accuracy: 96.73\n",
            "Epoch 8, Training Loss: 0.0037365397438406944, Validation Accuracy: 96.82\n",
            "Epoch 9, Training Loss: 0.0030077716801315546, Validation Accuracy: 96.82\n",
            "Epoch 10, Training Loss: 0.002476722002029419, Validation Accuracy: 96.91\n",
            "Epoch 11, Training Loss: 0.002099335426464677, Validation Accuracy: 97.03\n",
            "Epoch 12, Training Loss: 0.0017851145239546895, Validation Accuracy: 97.1\n",
            "Epoch 13, Training Loss: 0.0015034097013995051, Validation Accuracy: 97.25\n",
            "Epoch 14, Training Loss: 0.0012767505832016468, Validation Accuracy: 97.23\n",
            "Epoch 15, Training Loss: 0.00109112320933491, Validation Accuracy: 97.22\n",
            "Epoch 16, Training Loss: 0.0009178477921523154, Validation Accuracy: 97.22\n",
            "Epoch 17, Training Loss: 0.0007622911944054067, Validation Accuracy: 97.22\n",
            "Epoch 18, Training Loss: 0.0006246999255381525, Validation Accuracy: 97.22\n",
            "Epoch 19, Training Loss: 0.0004943578969687223, Validation Accuracy: 97.2\n",
            "Epoch 20, Training Loss: 0.0003766304289456457, Validation Accuracy: 97.29\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.29\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.29. Final Test Accuracy 96.9\n",
            "----------Max----------\n",
            "Epoch 1, Training Loss: 0.3097337484359741, Validation Accuracy: 89.85\n",
            "Epoch 2, Training Loss: 0.08535878360271454, Validation Accuracy: 92.7\n",
            "Epoch 3, Training Loss: 0.025965863838791847, Validation Accuracy: 94.5\n",
            "Epoch 4, Training Loss: 0.010718041099607944, Validation Accuracy: 95.55\n",
            "Epoch 5, Training Loss: 0.005569061730057001, Validation Accuracy: 96.12\n",
            "Epoch 6, Training Loss: 0.0034610864240676165, Validation Accuracy: 96.43\n",
            "Epoch 7, Training Loss: 0.002543787471950054, Validation Accuracy: 96.73\n",
            "Epoch 8, Training Loss: 0.00205972115509212, Validation Accuracy: 96.84\n",
            "Epoch 9, Training Loss: 0.001734658726491034, Validation Accuracy: 96.97\n",
            "Epoch 10, Training Loss: 0.0014993627555668354, Validation Accuracy: 97.12\n",
            "Epoch 11, Training Loss: 0.0013135385233908892, Validation Accuracy: 97.19\n",
            "Epoch 12, Training Loss: 0.0011325619416311383, Validation Accuracy: 97.23\n",
            "Epoch 13, Training Loss: 0.0009441685397177935, Validation Accuracy: 97.21\n",
            "Epoch 14, Training Loss: 0.0007630059262737632, Validation Accuracy: 97.27\n",
            "Epoch 15, Training Loss: 0.0006046851049177349, Validation Accuracy: 97.35\n",
            "Epoch 16, Training Loss: 0.00047910655848681927, Validation Accuracy: 97.37\n",
            "Epoch 17, Training Loss: 0.0003962923656217754, Validation Accuracy: 97.44\n",
            "Epoch 18, Training Loss: 0.0003486264031380415, Validation Accuracy: 97.54\n",
            "Epoch 19, Training Loss: 0.00031513971043750644, Validation Accuracy: 97.54\n",
            "Epoch 20, Training Loss: 0.0002754547167569399, Validation Accuracy: 97.57\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.57\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.57. Final Test Accuracy 97.55\n",
            "----------Weighted----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-9bbcc117a262>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetworkWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mperformance_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-fac8aaf5131a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Second hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----------Weighted----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkWeighted()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")\n",
        "\n",
        "print(\"----------Bias----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkBias()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AgT6lKlpsMx",
        "outputId": "2ed2e801-f24b-4bd4-a76c-d9520ba92e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Weighted----------\n",
            "Epoch 1, Training Loss: 0.1485009491443634, Validation Accuracy: 91.81\n",
            "Epoch 2, Training Loss: 0.038472242653369904, Validation Accuracy: 94.05\n",
            "Epoch 3, Training Loss: 0.0128360940143466, Validation Accuracy: 95.3\n",
            "Epoch 4, Training Loss: 0.005533022340387106, Validation Accuracy: 96.0\n",
            "Epoch 5, Training Loss: 0.003062085248529911, Validation Accuracy: 96.45\n",
            "Epoch 6, Training Loss: 0.001809746609069407, Validation Accuracy: 96.75\n",
            "Epoch 7, Training Loss: 0.0011564955348148942, Validation Accuracy: 96.99\n",
            "Epoch 8, Training Loss: 0.0008422164828516543, Validation Accuracy: 97.07\n",
            "Epoch 9, Training Loss: 0.0006858142442069948, Validation Accuracy: 97.17\n",
            "Epoch 10, Training Loss: 0.0005812147865071893, Validation Accuracy: 97.2\n",
            "Epoch 11, Training Loss: 0.00048744716332294047, Validation Accuracy: 97.3\n",
            "Epoch 12, Training Loss: 0.0004032037395518273, Validation Accuracy: 97.31\n",
            "Epoch 13, Training Loss: 0.00033158526639454067, Validation Accuracy: 97.33\n",
            "Epoch 14, Training Loss: 0.00027259447961114347, Validation Accuracy: 97.37\n",
            "Epoch 15, Training Loss: 0.00022837892174720764, Validation Accuracy: 97.48\n",
            "Epoch 16, Training Loss: 0.00019905969384126365, Validation Accuracy: 97.56\n",
            "Epoch 17, Training Loss: 0.00017677174764685333, Validation Accuracy: 97.62\n",
            "Epoch 18, Training Loss: 0.00015162272029556334, Validation Accuracy: 97.72\n",
            "Epoch 19, Training Loss: 0.00012575789878610522, Validation Accuracy: 97.75\n",
            "Epoch 20, Training Loss: 0.00010096516780322418, Validation Accuracy: 97.79\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.79\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.79. Final Test Accuracy 97.67\n",
            "----------Bias----------\n",
            "Epoch 1, Training Loss: 0.30999118089675903, Validation Accuracy: 90.39\n",
            "Epoch 2, Training Loss: 0.0559929683804512, Validation Accuracy: 93.41\n",
            "Epoch 3, Training Loss: 0.014778896234929562, Validation Accuracy: 95.19\n",
            "Epoch 4, Training Loss: 0.006188516039401293, Validation Accuracy: 95.92\n",
            "Epoch 5, Training Loss: 0.00329772662371397, Validation Accuracy: 96.42\n",
            "Epoch 6, Training Loss: 0.0020434230100363493, Validation Accuracy: 96.64\n",
            "Epoch 7, Training Loss: 0.0013054428854957223, Validation Accuracy: 96.89\n",
            "Epoch 8, Training Loss: 0.0008350699208676815, Validation Accuracy: 97.0\n",
            "Epoch 9, Training Loss: 0.0005414212355390191, Validation Accuracy: 97.06\n",
            "Epoch 10, Training Loss: 0.00034314466756768525, Validation Accuracy: 97.1\n",
            "Epoch 11, Training Loss: 0.00021360022947192192, Validation Accuracy: 97.23\n",
            "Epoch 12, Training Loss: 0.0001370812824461609, Validation Accuracy: 97.27\n",
            "Epoch 13, Training Loss: 9.07141511561349e-05, Validation Accuracy: 97.35\n",
            "Epoch 14, Training Loss: 5.9960475482512265e-05, Validation Accuracy: 97.44\n",
            "Epoch 15, Training Loss: 3.969590397900902e-05, Validation Accuracy: 97.52\n",
            "Epoch 16, Training Loss: 2.6702524337451905e-05, Validation Accuracy: 97.6\n",
            "Epoch 17, Training Loss: 1.8358061424805783e-05, Validation Accuracy: 97.68\n",
            "Epoch 18, Training Loss: 1.2874520507466514e-05, Validation Accuracy: 97.67\n",
            "Epoch 19, Training Loss: 9.179073458653875e-06, Validation Accuracy: 97.71\n",
            "Epoch 20, Training Loss: 6.794906312279636e-06, Validation Accuracy: 97.73\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.73\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.73. Final Test Accuracy 97.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----------BiasHalf----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkBiasHalf()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ER9la9lyXzh",
        "outputId": "b4188778-259d-4c16-da66-06bafd13e495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------BiasHalf----------\n",
            "Epoch 1, Training Loss: 0.2580236792564392, Validation Accuracy: 90.06\n",
            "Epoch 2, Training Loss: 0.056776389479637146, Validation Accuracy: 93.45\n",
            "Epoch 3, Training Loss: 0.017757480964064598, Validation Accuracy: 95.39\n",
            "Epoch 4, Training Loss: 0.008135632611811161, Validation Accuracy: 96.18\n",
            "Epoch 5, Training Loss: 0.004203413613140583, Validation Accuracy: 96.54\n",
            "Epoch 6, Training Loss: 0.002356254495680332, Validation Accuracy: 96.82\n",
            "Epoch 7, Training Loss: 0.0013949673157185316, Validation Accuracy: 96.95\n",
            "Epoch 8, Training Loss: 0.0008721124031580985, Validation Accuracy: 97.04\n",
            "Epoch 9, Training Loss: 0.0005750194541178644, Validation Accuracy: 97.08\n",
            "Epoch 10, Training Loss: 0.00040344204171560705, Validation Accuracy: 97.23\n",
            "Epoch 11, Training Loss: 0.0002992897352669388, Validation Accuracy: 97.28\n",
            "Epoch 12, Training Loss: 0.00023183519078884274, Validation Accuracy: 97.34\n",
            "Epoch 13, Training Loss: 0.00018416139937471598, Validation Accuracy: 97.39\n",
            "Epoch 14, Training Loss: 0.00014733182615600526, Validation Accuracy: 97.41\n",
            "Epoch 15, Training Loss: 0.00011574551899684593, Validation Accuracy: 97.4\n",
            "Epoch 16, Training Loss: 8.654219709569588e-05, Validation Accuracy: 97.43\n",
            "Epoch 17, Training Loss: 6.282132380874828e-05, Validation Accuracy: 97.38\n",
            "Epoch 18, Training Loss: 4.6132929128361866e-05, Validation Accuracy: 97.39\n",
            "Epoch 19, Training Loss: 3.40932747349143e-05, Validation Accuracy: 97.45\n",
            "Epoch 20, Training Loss: 2.539125671319198e-05, Validation Accuracy: 97.49\n",
            "Finished training with params {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Score: 97.49\n",
            "Best params: {'batch_size': 1, 'epochs': 20, 'learning_rate': 0.01}. Best Validation: 97.49. Final Test Accuracy 97.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter2 = {\n",
        "    'batch_size': [1], # 1, 2, 4, 8, 16, 32, 64\n",
        "    'epochs': [40], # 1, 2, 4, 5, 8, 10, 20\n",
        "    'learning_rate': [0.01], # 0.1, 0.02, 0.01, 0.005, 0.001\n",
        "}\n",
        "grid = ParameterGrid(parameter2)\n",
        "\n",
        "print(\"----------DefaultLong----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetwork()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWL2I8-Z8Xco",
        "outputId": "250724f5-73ae-4537-d52e-865d3a587261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------DefaultLong----------\n",
            "Epoch 1, Training Loss: 0.37175217270851135, Validation Accuracy: 90.1\n",
            "Epoch 2, Training Loss: 0.06012723967432976, Validation Accuracy: 93.59\n",
            "Epoch 3, Training Loss: 0.015364030376076698, Validation Accuracy: 95.32\n",
            "Epoch 4, Training Loss: 0.006601667497307062, Validation Accuracy: 96.06\n",
            "Epoch 5, Training Loss: 0.0038043521344661713, Validation Accuracy: 96.55\n",
            "Epoch 6, Training Loss: 0.0027267676778137684, Validation Accuracy: 96.74\n",
            "Epoch 7, Training Loss: 0.0021821276750415564, Validation Accuracy: 96.9\n",
            "Epoch 8, Training Loss: 0.001782139646820724, Validation Accuracy: 97.01\n",
            "Epoch 9, Training Loss: 0.0014360364293679595, Validation Accuracy: 97.16\n",
            "Epoch 10, Training Loss: 0.0011397063499316573, Validation Accuracy: 97.33\n",
            "Epoch 11, Training Loss: 0.0008917645900510252, Validation Accuracy: 97.4\n",
            "Epoch 12, Training Loss: 0.000685576000250876, Validation Accuracy: 97.48\n",
            "Epoch 13, Training Loss: 0.00052426423644647, Validation Accuracy: 97.49\n",
            "Epoch 14, Training Loss: 0.0004107108688913286, Validation Accuracy: 97.49\n",
            "Epoch 15, Training Loss: 0.0003389737685211003, Validation Accuracy: 97.51\n",
            "Epoch 16, Training Loss: 0.000296310376143083, Validation Accuracy: 97.57\n",
            "Epoch 17, Training Loss: 0.0002656822034623474, Validation Accuracy: 97.59\n",
            "Epoch 18, Training Loss: 0.00022539935889653862, Validation Accuracy: 97.56\n",
            "Epoch 19, Training Loss: 0.00017438798386137933, Validation Accuracy: 97.58\n",
            "Epoch 20, Training Loss: 0.000125281119835563, Validation Accuracy: 97.61\n",
            "Epoch 21, Training Loss: 8.606540359323844e-05, Validation Accuracy: 97.66\n",
            "Epoch 22, Training Loss: 5.829164365422912e-05, Validation Accuracy: 97.71\n",
            "Epoch 23, Training Loss: 4.053033626405522e-05, Validation Accuracy: 97.71\n",
            "Epoch 24, Training Loss: 2.932505594799295e-05, Validation Accuracy: 97.71\n",
            "Epoch 25, Training Loss: 2.2172682292875834e-05, Validation Accuracy: 97.74\n",
            "Epoch 26, Training Loss: 1.764281842042692e-05, Validation Accuracy: 97.73\n",
            "Epoch 27, Training Loss: 1.4543427823809907e-05, Validation Accuracy: 97.75\n",
            "Epoch 28, Training Loss: 1.2397689715726301e-05, Validation Accuracy: 97.78\n",
            "Epoch 29, Training Loss: 1.07287787614041e-05, Validation Accuracy: 97.82\n",
            "Epoch 30, Training Loss: 9.179073458653875e-06, Validation Accuracy: 97.83\n",
            "Epoch 31, Training Loss: 7.986990567587782e-06, Validation Accuracy: 97.83\n",
            "Epoch 32, Training Loss: 6.794906312279636e-06, Validation Accuracy: 97.82\n",
            "Epoch 33, Training Loss: 5.8412379075889476e-06, Validation Accuracy: 97.88\n",
            "Epoch 34, Training Loss: 5.006777428206988e-06, Validation Accuracy: 97.88\n",
            "Epoch 35, Training Loss: 4.410734163684538e-06, Validation Accuracy: 97.89\n",
            "Epoch 36, Training Loss: 3.814689989667386e-06, Validation Accuracy: 97.89\n",
            "Epoch 37, Training Loss: 3.3378546504536644e-06, Validation Accuracy: 97.89\n",
            "Epoch 38, Training Loss: 2.9802276912960224e-06, Validation Accuracy: 97.89\n",
            "Epoch 39, Training Loss: 2.622600959512056e-06, Validation Accuracy: 97.95\n",
            "Epoch 40, Training Loss: 2.264974000354414e-06, Validation Accuracy: 97.95\n",
            "Finished training with params {'batch_size': 1, 'epochs': 40, 'learning_rate': 0.01}. Score: 97.95\n",
            "Best params: {'batch_size': 1, 'epochs': 40, 'learning_rate': 0.01}. Best Validation: 97.95. Final Test Accuracy 97.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter2 = {\n",
        "    'batch_size': [1], # 1, 2, 4, 8, 16, 32, 64\n",
        "    'epochs': [50], # 1, 2, 4, 5, 8, 10, 20\n",
        "    'learning_rate': [0.01], # 0.1, 0.02, 0.01, 0.005, 0.001\n",
        "}\n",
        "grid = ParameterGrid(parameter2)\n",
        "\n",
        "print(\"----------Three----------\")\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_model = None\n",
        "for params in grid:\n",
        "    model = NeuralNetworkThree()\n",
        "    performance_metric, curr_model = train_and_evaluate_model(params, model)\n",
        "\n",
        "    if best_score is None or performance_metric > best_score:\n",
        "        best_score = performance_metric\n",
        "        best_params = params\n",
        "        best_model = curr_model\n",
        "\n",
        "    print(f\"Finished training with params {params}. Score: {performance_metric}\")\n",
        "test_accuracy = get_accuracy(best_model, \"t\")\n",
        "print(f\"Best params: {best_params}. Best Validation: {best_score}. Final Test Accuracy {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "8_FzvNgcVRcD",
        "outputId": "8c5cbcb7-fe1c-41aa-bdfe-294b85b0e788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Three----------\n",
            "Epoch 1, Training Loss: 2.453505039215088, Validation Accuracy: 20.03\n",
            "Epoch 2, Training Loss: 0.5540778636932373, Validation Accuracy: 84.86\n",
            "Epoch 3, Training Loss: 0.135878324508667, Validation Accuracy: 90.05\n",
            "Epoch 4, Training Loss: 0.04672578349709511, Validation Accuracy: 93.34\n",
            "Epoch 5, Training Loss: 0.02098885551095009, Validation Accuracy: 95.15\n",
            "Epoch 6, Training Loss: 0.010926534421741962, Validation Accuracy: 95.81\n",
            "Epoch 7, Training Loss: 0.006259716581553221, Validation Accuracy: 96.32\n",
            "Epoch 8, Training Loss: 0.003958367742598057, Validation Accuracy: 96.45\n",
            "Epoch 9, Training Loss: 0.0025081150233745575, Validation Accuracy: 96.58\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-6a227b332f3d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetworkThree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mperformance_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mperformance_metric\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-f5d6be7a2db0>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(params, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mmomentum_buffer_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mhas_sparse_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             sgd(params_with_grad,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, d_p_list, momentum_buffer_list)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0md_p_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}